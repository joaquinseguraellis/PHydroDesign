{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<font size=\"500\">Precipitation Analysis</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --user --upgrade --force-reinstall \"git+https://github.com/joaquinseguraellis/PydroDesign.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RiSA.basics import *\n",
    "from RiSA.download import *\n",
    "from RiSA.frequency_analysis import *\n",
    "from RiSA.geo_tools import *\n",
    "from RiSA.hydro_tools import *\n",
    "from RiSA.interp import *\n",
    "from RiSA.masks import *\n",
    "from RiSA.figures import *\n",
    "from RiSA.results import *\n",
    "from RiSA.rg_pre import *\n",
    "from RiSA import __version__\n",
    "print(f'RiSA version: {__version__.version()}')\n",
    "\n",
    "download_credentials(\n",
    "    urs='urs.earthdata.nasa.gov',\n",
    "    username='joako19',\n",
    "    password='$Q#9mqZ%)ZV5jDt'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main:\n",
    "    \"\"\"\n",
    "    Main execution class for the project\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.start_date = datetime.datetime(2000, 7, 1)\n",
    "        self.end_date   = datetime.datetime(2021, 7, 1)\n",
    "        self.bbox = [-76.05, -53.05, -55.95, -21.05] # lon1, lon2, lat1, lat2\n",
    "        self.lon_IMERG = np.arange(self.bbox[0], self.bbox[1] + 0.001, 0.1)\n",
    "        self.lat_IMERG = np.arange(self.bbox[2], self.bbox[3] + 0.001, 0.1)\n",
    "        self.period = ['Y', 'jja', 'son', 'djf', 'mam']\n",
    "        self.T = np.arange(2, 50.1, 1)\n",
    "        self.error = list()\n",
    "        self.stations = list()\n",
    "        self.test_w_out = list()\n",
    "        self.test = list()\n",
    "        self.for_use_w_out = list()\n",
    "        self.for_use = list()\n",
    "\n",
    "    def create_paths(\n",
    "            self, stations_dir, rg_bin_dir, imerg_bin_dir,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.stations_dir = stations_dir\n",
    "        self.outputs_dir = Path('outputs')\n",
    "        if not os.path.exists(self.outputs_dir):\n",
    "            os.mkdir(self.outputs_dir)\n",
    "        self.card_dir = Path(self.outputs_dir, f'cards_Y_rx1day')\n",
    "        if not os.path.exists(self.card_dir):\n",
    "            os.mkdir(self.card_dir)\n",
    "        self.bin_dir = Path('bin')\n",
    "        if not os.path.exists(self.bin_dir):\n",
    "            os.mkdir(self.bin_dir)\n",
    "        self.rg_bin_dir = Path(self.bin_dir, rg_bin_dir)\n",
    "        if not os.path.exists(self.rg_bin_dir):\n",
    "            os.mkdir(self.rg_bin_dir)\n",
    "        self.imerg_bin_dir = Path(self.bin_dir, imerg_bin_dir)\n",
    "        if not os.path.exists(self.imerg_bin_dir):\n",
    "            os.mkdir(self.imerg_bin_dir)\n",
    "\n",
    "    def get_topo(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        topo, _ = etopo(\n",
    "            self.lon_IMERG, self.lat_IMERG,\n",
    "            Path(Path(os.getcwd()).parent, 'EarthData'),\n",
    "            engine='scipy',\n",
    "        )\n",
    "        return topo\n",
    "    \n",
    "    def download(\n",
    "            self, path,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ***Beware***, it may change over time.\n",
    "        \"\"\"\n",
    "        start_dt = datetime.datetime.strftime(self.start_date, '%Y-%m-%d')\n",
    "        end_dt = datetime.datetime.strftime(self.end_date, '%Y-%m-%d')\n",
    "        dts = np.array(\n",
    "            [\n",
    "                start_dt + datetime.timedelta(days=day)\n",
    "                for day in range((end_dt - start_dt).days)\n",
    "            ]\n",
    "        )\n",
    "        years = np.array([_.year for _ in dts])\n",
    "        products = ['Early', 'Late', 'Final']\n",
    "        freq = '30T'\n",
    "        for product in products:\n",
    "            print(f'Downloading - {product} - ...')\n",
    "            save_path = Path(path, f'temp_{product}_{freq}')\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_path)\n",
    "            for year in np.unique(years):\n",
    "                s_dt = np.min(dts[years == year])\n",
    "                e_dt = np.max(dts[years == year])\n",
    "                save_path_ = Path(save_path, str(year))\n",
    "                if not os.path.exists(save_path_):\n",
    "                    os.mkdir(save_path_)\n",
    "                download = Download_IMERG(\n",
    "                    freq,\n",
    "                    f'{product}',\n",
    "                    self.bbox, # lon1, lon2, lat1, lat2\n",
    "                    save_path_,\n",
    "                )\n",
    "                download.download(\n",
    "                    f'{year}-{s_dt.month}-{s_dt.day}',\n",
    "                    f'{year}-{e_dt.month}-{e_dt.day}',\n",
    "                )\n",
    "    \n",
    "    def create_file_part(\n",
    "            self, path, file_paths, time_index,\n",
    "            time_size, t, product, start_end_hour,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        def write_dataset(hdf_file, dataset, ds):\n",
    "            hdf_file.create_dataset(name=dataset, data=ds)\n",
    "            hdf_file[dataset].attrs['missing_value'] = 'NaN'\n",
    "            hdf_file[dataset].attrs['missing_value_type'] = 'numpy.nan'\n",
    "            hdf_file[dataset].attrs['dimension_names'] = ['time', 'lon', 'lat']\n",
    "            hdf_file[dataset].attrs['dimension_coordinates'] = [0, 1, 2]\n",
    "            hdf_file[dataset].attrs['original_name'] = dataset\n",
    "            hdf_file[dataset].attrs['full_name'] = f'{dataset} 24 hours accumulated'\n",
    "            hdf_file[dataset].attrs['units'] = 'mm'\n",
    "        if not os.path.exists(path):\n",
    "            print(path)\n",
    "            time_index_ = time_index + time_size\n",
    "            if time_index_ > t.shape[0]:\n",
    "                time_index_ = t.shape[0]\n",
    "                precipitationCal, precipitationUncal, IRprecipitation, HQprecipitation = get_imerg_daily_sum(file_paths[time_index:], self.bbox)\n",
    "            else:\n",
    "                precipitationCal, precipitationUncal, IRprecipitation, HQprecipitation = get_imerg_daily_sum(file_paths[time_index:time_index_], self.bbox)\n",
    "            with h5py.File(path, 'w') as hdf_file:\n",
    "                \"\"\"File Information\"\"\"\n",
    "                hdf_file.attrs['file_author'] = 'Segura Ellis, Joaquin Sebastian'\n",
    "                hdf_file.attrs['contact_mail'] = 'joaquin.segura.ellis@mi.unc.edu.ar'\n",
    "                hdf_file.attrs['creation_time'] = str(datetime.datetime.now(tz=datetime.timezone.utc))\n",
    "                hdf_file.attrs['description'] = f'This file is based on IMERG {product} half-hourly datasets. It is a daily accumulated precipitation dataset. Every single value has 3 coordinates, the time represents an accumulation from 24 hours before, the longitude and the latitude.'\n",
    "                hdf_file.attrs['start_end_hour_UTC'] = start_end_hour\n",
    "                \"\"\"precipitationCal dataset and its information\"\"\"\n",
    "                write_dataset(hdf_file, 'precipitationCal', precipitationCal)\n",
    "                \"\"\"precipitationUncal dataset and its information\"\"\"\n",
    "                # write_dataset(hdf_file, 'precipitationUncal', precipitationUncal)\n",
    "                \"\"\"IRprecipitation dataset and its information\"\"\"\n",
    "                # write_dataset(hdf_file, 'IRprecipitation', IRprecipitation)\n",
    "                \"\"\"HQprecipitation dataset and its information\"\"\"\n",
    "                # write_dataset(hdf_file, 'HQprecipitation', HQprecipitation)\n",
    "                \"\"\"longitude dataset and its information\"\"\"\n",
    "                hdf_file.create_dataset(name='/lon', data=self.lon_IMERG)\n",
    "                hdf_file['lon'].attrs['coordinate'] = 1\n",
    "                hdf_file['lon'].attrs['axis'] = 'X'\n",
    "                hdf_file['lon'].attrs['bounds'] = self.lon_IMERG[[0, -1]]\n",
    "                hdf_file['lon'].attrs['standard_name'] = 'longitude'\n",
    "                hdf_file['lon'].attrs['long_name'] = 'Longitude at the center of a 0.10 degree grid'\n",
    "                hdf_file['lon'].attrs['units'] = 'degrees_east'\n",
    "                \"\"\"latitude dataset and its information\"\"\"\n",
    "                hdf_file.create_dataset(name='/lat', data=self.lat_IMERG)\n",
    "                hdf_file['lat'].attrs['coordinate'] = 2\n",
    "                hdf_file['lat'].attrs['axis'] = 'Y'\n",
    "                hdf_file['lat'].attrs['bounds'] = self.lat_IMERG[[0, -1]]\n",
    "                hdf_file['lat'].attrs['standard_name'] = 'latitude'\n",
    "                hdf_file['lat'].attrs['long_name'] = 'Latitude at the center of a 0.10 degree grid'\n",
    "                hdf_file['lat'].attrs['units'] = 'degrees_north'\n",
    "                \"\"\"time dataset and its information\"\"\"\n",
    "                hdf_file.create_dataset(name='/time', data=np.arange(0, t.shape[0])[time_index:time_index_])\n",
    "                hdf_file['time'].attrs['coordinate'] = 0\n",
    "                hdf_file['time'].attrs['axis'] = 'T'\n",
    "                hdf_file['time'].attrs['bounds'] = [time_index, time_index_ - 1]\n",
    "                hdf_file['time'].attrs['calendar'] = 'julian'\n",
    "                hdf_file['time'].attrs['standard_name'] = 'time'\n",
    "                hdf_file['time'].attrs['long_name'] = 'Representative time of data'\n",
    "                hdf_file['time'].attrs['description'] = 'Data is accumulated between this time and 24 hours before'\n",
    "                hdf_file['time'].attrs['units'] = f'days since {str(t[0])}'\n",
    "                hdf_file['time'].attrs['first_time'] = str(t[0])\n",
    "                hdf_file['time'].attrs['format_Python_datetime_library'] = '%Y:%m:%d %H:%M'\n",
    "                hdf_file['time'].attrs['time_zone'] = 'UTC'\n",
    "\n",
    "    def gridded_daily_rainfall(\n",
    "            self, data_path, save_path, start_end_hour, product,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        file_paths, t = np.array(\n",
    "            [\n",
    "                [\n",
    "                    Path(data_path, year, file),\n",
    "                    datetime.datetime.strptime(\n",
    "                        file[:file.index('.')], '%Y%m%d%H%M',\n",
    "                    ),\n",
    "                ]\n",
    "                for year in os.listdir(data_path)\n",
    "                for file in os.listdir(Path(data_path, year))\n",
    "            ]\n",
    "        ).T\n",
    "        first_time = datetime.datetime(\n",
    "            t[0].year, t[0].month, t[0].day, start_end_hour, 30,\n",
    "        )\n",
    "        file_paths = file_paths[t >= first_time]\n",
    "        file_paths = np.array([\n",
    "            file_paths[i-48:i]\n",
    "            for i in range(48, file_paths.shape[0], 48)\n",
    "        ])\n",
    "        t = t[t >= first_time]\n",
    "        t = np.array([t[i] for i in range(47, t.shape[0], 48)])\n",
    "        time_size = 300\n",
    "        part = 1\n",
    "        if not os.path.exists(Path(save_path, f'IMERG_{product}_{start_end_hour}')):\n",
    "            os.mkdir(Path(save_path, f'IMERG_{product}_{start_end_hour}'))\n",
    "        for time_index in range(0, t.shape[0], time_size):\n",
    "            self.create_file_part(\n",
    "                Path(\n",
    "                    save_path,\n",
    "                    f'IMERG_{product}_{start_end_hour}',\n",
    "                    f'IMERG_{product}_{start_end_hour}_part_{part}.hdf5',\n",
    "                ),\n",
    "                file_paths, time_index, time_size, t, product, start_end_hour,\n",
    "            )\n",
    "            part += 1\n",
    "\n",
    "    def get_station(\n",
    "            self, file,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        ind_k = 'Y_rx1day'\n",
    "        name = file[:file.find('.')]\n",
    "        station = Rain_Gauge()\n",
    "        station.load(Path(self.stations_dir, file))\n",
    "        station.file = name\n",
    "        station.period = self.period\n",
    "        station.result = dict()\n",
    "        station.data = station.data[station.time < self.end_date]\n",
    "        station.time = station.time[station.time < self.end_date]\n",
    "        station.data = station.data[station.time >= self.start_date]\n",
    "        station.time = station.time[station.time >= self.start_date]\n",
    "        if station.time.shape[0] > 0:\n",
    "            station.rxDday_calc(1)\n",
    "            bbox_condition = inside_bbox(\n",
    "                self.bbox, station.lat, station.lon,\n",
    "            )\n",
    "            min_condition = minimum_lenght(\n",
    "                station.result[ind_k]['data'], 14,\n",
    "            )\n",
    "            elev_condition = station.elevation < 3000\n",
    "            if bbox_condition and min_condition and elev_condition:\n",
    "                station.sorted_max_calc()\n",
    "                station = frequency_analysis(station, replace=1)\n",
    "                return station\n",
    "        \n",
    "    def get_imerg(\n",
    "            self, station, imerg, idw,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        imerg_data = imerg.data\n",
    "        imerg_data = idw.interp(imerg_data, axes=[0, 2, 1])\n",
    "        imerg_dt, index1, index2 = np.intersect1d(\n",
    "            imerg.time, station.time, return_indices=True,\n",
    "        )\n",
    "        imerg_data = imerg_data[index1]\n",
    "        imerg_data[np.isnan(station.data[index2])] = np.nan\n",
    "        imerg = Rainfall_Indices(\n",
    "            imerg_dt, imerg_data, start_month=station.start_month,\n",
    "        )\n",
    "        imerg.rxDday_calc(1)\n",
    "        imerg.sorted_max_calc()\n",
    "        imerg = frequency_analysis(imerg, replace=1)\n",
    "        return imerg\n",
    "    \n",
    "    def get_analysis(\n",
    "            self, file, imerg,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        station = self.get_station(file)\n",
    "        if station is not None:\n",
    "            idw = IDW_Grid_Interpolation(\n",
    "                self.lon_IMERG, self.lat_IMERG,\n",
    "                station.lon, station.lat, 2,\n",
    "            )\n",
    "            imerg_e = self.get_imerg(\n",
    "                station, imerg['Early'][int(station.record_time[:2])],\n",
    "                idw,\n",
    "            )\n",
    "            imerg_l = self.get_imerg(\n",
    "                station, imerg['Late'][int(station.record_time[:2])],\n",
    "                idw,\n",
    "            )\n",
    "            idw = IDW_Grid_Interpolation(\n",
    "                self.lon_IMERG, self.lat_IMERG,\n",
    "                station.lon, station.lat, 3,\n",
    "            )\n",
    "            imerg_f = self.get_imerg(\n",
    "                station, imerg['Final'][int(station.record_time[:2])],\n",
    "                idw,\n",
    "            )\n",
    "            return station, imerg_e, imerg_l, imerg_f\n",
    "\n",
    "    def create_card(\n",
    "            self, station, save_path, imerg_e, imerg_l, imerg_f,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        ind_k = 'Y_rx1day'\n",
    "        card = Card_IMERG([-58, -76, -20, -53], 10, station, save_path)\n",
    "        if not os.path.exists(Path(card.save_path, f'{card.station.file}.png')):\n",
    "            card.create_figure(\n",
    "                title=f'{card.station.file} - {card.station.province}',\n",
    "                figsize=(19.2, 10.8), dpi=100,\n",
    "            )\n",
    "            card.add_imerg(imerg_e, imerg_l, imerg_f)\n",
    "            card.draw_map1(card.ax_map)\n",
    "            card.draw_plot1(card.ax_plot1)\n",
    "            card.draw_plot2(\n",
    "                card.ax_plot2,\n",
    "                card.station.result['Y_3_sorted_rx1day']['time'],\n",
    "                card.station.result[ind_k]['outliers'],\n",
    "            )\n",
    "            card.draw_plot3(card.ax_plot3)\n",
    "            card.draw_plot4(card.ax_plot4, self.T)\n",
    "            card.draw_table1(\n",
    "                card.ax_info1,\n",
    "                card.station.result[ind_k]['data_outliers_tests'],\n",
    "                card.station.result[ind_k]['outliers'],\n",
    "            )\n",
    "            card.add_to_table1(self.T)\n",
    "            card.fig.tight_layout()\n",
    "            card.fig.autofmt_xdate()\n",
    "            plt.savefig(Path(card.save_path, f'{card.station.file}.png'))\n",
    "            plt.close()\n",
    "    \n",
    "    def fill_interp_imerg(\n",
    "            self, data, mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            data[mask] = np.nan\n",
    "        X, Y = np.meshgrid(self.lon_IMERG, self.lat_IMERG)\n",
    "        points = np.array([X.flatten(), Y.flatten()]).T\n",
    "        data = data.flatten()\n",
    "        points = points[~np.isnan(data)]\n",
    "        data = data[~np.isnan(data)]\n",
    "        return scipy.interpolate.griddata(\n",
    "            points, data, (X, Y), method='linear',\n",
    "        )\n",
    "\n",
    "    def get_results(\n",
    "            self, argentina_shp_path,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        ind_k = 'Y_rx1day'\n",
    "        wo_k = 'data_without_outliers'\n",
    "        ot_k = 'data_outliers_tests'\n",
    "        T = [2, 5, 10, 25, 50]\n",
    "        elevation_mask = self.topo >= 3000\n",
    "        station_i = np.arange(len(self.stations))[\n",
    "            np.array([_[0].name for _ in self.stations]) == 'Esquel Aero'\n",
    "        ][0]\n",
    "        tests_examples(\n",
    "            Path(self.outputs_dir, '01_tests_examples.png'),\n",
    "        )\n",
    "        map_station_institution(\n",
    "            self.bbox,\n",
    "            np.array([_[0].lon for _ in self.stations]),\n",
    "            np.array([_[0].lat for _ in self.stations]),\n",
    "            np.array([_[0].institution for _ in self.stations]),\n",
    "            Path(self.outputs_dir, '01_institutions.png'),\n",
    "        )\n",
    "        hist_information(\n",
    "            np.array([_[0].institution for _ in self.stations]),\n",
    "            np.array([_[0].lon for _ in self.stations]),\n",
    "            np.array([_[0].lat for _ in self.stations]),\n",
    "            np.array([_[0].elevation for _ in self.stations]),\n",
    "            self.for_use_w_out,\n",
    "            Path(self.outputs_dir, '01_information.png'),\n",
    "        )    \n",
    "        map_start_month(\n",
    "            self.bbox,\n",
    "            np.array([_[0].lon for _ in self.stations]),\n",
    "            np.array([_[0].lat for _ in self.stations]),\n",
    "            np.array([_[0].start_month for _ in self.stations]),\n",
    "            Path(self.outputs_dir, '02_start_month.png'),\n",
    "        )\n",
    "        interpolation_method_comparison(\n",
    "            Path(self.outputs_dir, 'interpolations.csv'),\n",
    "            Path(self.outputs_dir, '04_interpolation_comparison.png'),\n",
    "        )\n",
    "        scatter_station_imerg(\n",
    "            self.stations[station_i][0].data,\n",
    "            self.stations[station_i][1].data,\n",
    "            self.stations[station_i][2].data,\n",
    "            self.stations[station_i][3].data,\n",
    "            self.stations[station_i][0].name,\n",
    "            Path(self.outputs_dir, '05_example_daily_data.png'),\n",
    "        )\n",
    "        scatter_station_imerg(\n",
    "            self.stations[station_i][0].result[ind_k]['outliers'][wo_k][-1],\n",
    "            self.stations[station_i][1].result[ind_k]['outliers'][wo_k][-1],\n",
    "            self.stations[station_i][2].result[ind_k]['outliers'][wo_k][-1],\n",
    "            self.stations[station_i][3].result[ind_k]['outliers'][wo_k][-1],\n",
    "            self.stations[station_i][0].name,\n",
    "            Path(self.outputs_dir, '06_example_rx1day.png'),\n",
    "        )\n",
    "        map_station_test(\n",
    "            self.bbox, np.array([_[0].lon for _ in self.stations]), np.array([_[0].lat for _ in self.stations]),\n",
    "            Path(self.outputs_dir, '07_station_tests.png'),\n",
    "            np.array([\n",
    "                np.array(self.test_w_out, dtype=bool)[:, 0],\n",
    "                np.array([\n",
    "                    minimum_lenght(_[0].result[ind_k]['outliers'][wo_k], 14)\n",
    "                    for _ in self.stations\n",
    "                ]),\n",
    "                np.array([\n",
    "                    _[0].result[ind_k][ot_k]['iWW']['index']\n",
    "                    for _ in self.stations\n",
    "                ]) != 3,\n",
    "                np.array([\n",
    "                    _[0].result[ind_k][ot_k]['tMK']['index'][0]\n",
    "                    for _ in self.stations\n",
    "                ]) == 1,\n",
    "                np.array([\n",
    "                    _[0].result[ind_k][ot_k]['tMK']['index'][0]\n",
    "                    for _ in self.stations\n",
    "                ]) == 3,\n",
    "                np.array([\n",
    "                    int(_[0].result[ind_k][ot_k]['hPE']['index'])\n",
    "                    for _ in self.stations\n",
    "                ]) != 3,\n",
    "            ]),\n",
    "        )\n",
    "        for t in T:\n",
    "            map_prec(\n",
    "                self.bbox, self.lon_IMERG, self.lat_IMERG,\n",
    "                np.array([_[0].lon for _ in self.stations])[self.for_use_w_out],\n",
    "                np.array([_[0].lat for _ in self.stations])[self.for_use_w_out],\n",
    "                np.array([\n",
    "                    self.stations[j][0].result[ind_k]['lognorm'].ppf([t])[1]\n",
    "                        for j in range(len(self.stations)) if self.for_use_w_out[j]\n",
    "                ]),\n",
    "                Path(self.outputs_dir, f'09_stations_{t}_years.png'),\n",
    "                shp_path=argentina_shp_path, elevation_mask=elevation_mask,\n",
    "            )\n",
    "            map_Catalini_comp(\n",
    "                self.bbox, self.lon_IMERG, self.lat_IMERG,\n",
    "                np.array([_[0].lon for _ in self.stations])[self.for_use_w_out],\n",
    "                np.array([_[0].lat for _ in self.stations])[self.for_use_w_out],\n",
    "                np.array([\n",
    "                    self.stations[j][0].result[ind_k]['lognorm'].ppf([t])[1]\n",
    "                        for j in range(len(self.stations)) if self.for_use_w_out[j]\n",
    "                ]),\n",
    "                Path(self.outputs_dir, f'14_catalini_comp_{t}_years.png'),\n",
    "                f'C:\\\\Users\\\\joako\\\\OneDrive\\\\1 - Posgrado\\\\Workspace\\\\IMERG\\\\arg_data_img\\\\pmd{t}.tif',\n",
    "                Path(Path(os.getcwd()).parent, 'IMERG', 'arg_data_img', 'Estaciones', 'Estaciones.shp'),\n",
    "                shp_path=argentina_shp_path, elevation_mask=elevation_mask,\n",
    "            )\n",
    "        bounds = [\n",
    "            None,\n",
    "            np.array([\n",
    "                0, 1, 2, 5, 10, 15, 20, 25,\n",
    "                30, 35, 40, 45, 50, 60, 70,\n",
    "            ]),\n",
    "            np.array([\n",
    "                0, 1, 2, 5, 10, 15, 20, 25,\n",
    "                30, 35, 40, 45, 50, 60, 70,\n",
    "            ]),\n",
    "            np.array([\n",
    "                0, 50, 100, 150, 200, 250, 300, 350,\n",
    "                400, 450, 500, 550, 600, 700, 800,\n",
    "            ]),\n",
    "            np.array([\n",
    "                0.20, 0.24, 0.28, 0.32, 0.36, 0.40, 0.44, 0.48,\n",
    "                0.52, 0.56, 0.60, 0.70, 0.80, 1.00, 2.00,\n",
    "            ]),\n",
    "        ]\n",
    "        cb_label = [\n",
    "            'Precipitación (mm/día)', 'Precipitación (mm/día)',\n",
    "            '', 'Precipitación (mm/día)', '',\n",
    "        ]\n",
    "        for i, key in enumerate(['data_mean', 'data_std', 'phi_pmp', 'pmp']):\n",
    "            map_prec(\n",
    "                self.bbox, self.lon_IMERG, self.lat_IMERG,\n",
    "                np.array([_[0].lon for _ in self.stations])[self.for_use],\n",
    "                np.array([_[0].lat for _ in self.stations])[self.for_use],\n",
    "                np.array([\n",
    "                    self.stations[j][0].result[ind_k][key]\n",
    "                        for j in range(len(self.stations)) if self.for_use[j]\n",
    "                ]),\n",
    "                Path(self.outputs_dir, f'12_stations_{key}.png'),\n",
    "                bounds=bounds[i],\n",
    "                shp_path=argentina_shp_path, elevation_mask=elevation_mask,\n",
    "                cb_label=cb_label[i],\n",
    "            )\n",
    "        map_prec(\n",
    "            self.bbox, self.lon_IMERG, self.lat_IMERG,\n",
    "            np.array([_[0].lon for _ in self.stations])[self.for_use],\n",
    "            np.array([_[0].lat for _ in self.stations])[self.for_use],\n",
    "            np.array([\n",
    "                self.stations[j][0].result[ind_k]['data_std'] / \\\n",
    "                self.stations[j][0].result[ind_k]['data_mean']\n",
    "                for j in range(len(self.stations))\n",
    "                if self.for_use[j]\n",
    "            ]),\n",
    "            Path(self.outputs_dir, '12_stations_data_cv.png'),\n",
    "            bounds=bounds[-1],\n",
    "            shp_path=argentina_shp_path, elevation_mask=elevation_mask,\n",
    "            cb_label=cb_label[-1],\n",
    "        )\n",
    "        for p in ['Early', 'Late', 'Final']:\n",
    "            rt = 12\n",
    "            map_imerg_start_month(\n",
    "                self.bbox, self.imerg[p][rt].lon, self.imerg[p][rt].lat,\n",
    "                self.imerg[p][rt].start_month.T,\n",
    "                Path(self.outputs_dir, f'03_imerg_start_month_{p}_{rt}.png'),\n",
    "                argentina_shp_path, elevation_mask,\n",
    "            )\n",
    "            tests = self.imerg[p][rt].result[ind_k]['tests_data_without_outliers']\n",
    "            cond_iWW = tests['iWW']['index'].T == 3\n",
    "            cond_hPE = tests['hPE']['index'].T == '3'\n",
    "            cond_tMK = tests['tMK']['index'][:, :, 0].T == 2\n",
    "            cond = (cond_iWW * cond_hPE * cond_tMK).astype(bool)\n",
    "            map_imerg_test(\n",
    "                self.bbox, self.imerg[p][rt].lon, self.imerg[p][rt].lat,\n",
    "                cond_iWW, cond_tMK, cond_hPE,\n",
    "                Path(self.outputs_dir, f'08_imerg_tests_{p}_{rt}.png'),\n",
    "                argentina_shp_path, elevation_mask,\n",
    "            )\n",
    "            for i, t in enumerate(T):\n",
    "                prec = self.imerg[p][rt].result[ind_k]['lognorm'][:, :, 1, i].T\n",
    "                if t == 10 and p == 'Final':\n",
    "                    map_imerg_prec(\n",
    "                        self.bbox, self.imerg[p][rt].lon,\n",
    "                        self.imerg[p][rt].lat,\n",
    "                        np.where(cond, prec, 0),\n",
    "                        Path(\n",
    "                            self.outputs_dir,\n",
    "                            f'10_imerg_prec_{p}_{rt}_{t}_incomplete.png',\n",
    "                        ),\n",
    "                        shp_path=argentina_shp_path,\n",
    "                        elevation_mask=elevation_mask,\n",
    "                    )\n",
    "                    map_imerg_prec(\n",
    "                        self.bbox, self.imerg[p][rt].lon, self.imerg[p][rt].lat, prec,\n",
    "                        Path(\n",
    "                            self.outputs_dir,\n",
    "                            f'10_imerg_prec_{p}_{rt}_{t}_complete.png',\n",
    "                        ),\n",
    "                        shp_path=argentina_shp_path,\n",
    "                        elevation_mask=elevation_mask,\n",
    "                    )\n",
    "                map_imerg_prec(\n",
    "                    self.bbox, self.imerg[p][rt].lon, self.imerg[p][rt].lat,\n",
    "                    self.fill_interp_imerg(prec, ~cond),\n",
    "                    Path(\n",
    "                        self.outputs_dir,\n",
    "                        f'11_imerg_prec_{p}_{rt}_{t}_years.png',\n",
    "                    ),\n",
    "                    shp_path=argentina_shp_path,\n",
    "                    elevation_mask=elevation_mask,\n",
    "                )\n",
    "            tests = self.imerg[p][rt].result[ind_k]['tests_data']\n",
    "            cond_iWW = tests['iWW']['index'].T == 3\n",
    "            cond_hPE = tests['hPE']['index'].T == '3'\n",
    "            cond_tMK = tests['tMK']['index'][:, :, 0].T == 2\n",
    "            cond = (cond_iWW * cond_hPE * cond_tMK).astype(bool)\n",
    "            for i, key in enumerate(['mean', 'std', 'phi_pmp', 'pmp']):\n",
    "                map_imerg_prec(\n",
    "                    self.bbox, self.imerg[p][rt].lon, self.imerg[p][rt].lat,\n",
    "                    self.fill_interp_imerg(\n",
    "                        self.imerg[p][rt].result[ind_k][key].T, ~cond,\n",
    "                    ),\n",
    "                    Path(self.outputs_dir, f'13_imerg_prec_{p}_{rt}_{key}.png'),\n",
    "                    bounds=bounds[i],\n",
    "                    shp_path=argentina_shp_path,\n",
    "                    elevation_mask=elevation_mask,\n",
    "                    cb_label=cb_label[i],\n",
    "                )\n",
    "            map_imerg_prec(\n",
    "                self.bbox, self.imerg[p][rt].lon, self.imerg[p][rt].lat,\n",
    "                self.fill_interp_imerg(\n",
    "                    self.imerg[p][rt].result[ind_k]['std'].T / \\\n",
    "                    self.imerg[p][rt].result[ind_k]['mean'].T,\n",
    "                    ~cond,\n",
    "                ),\n",
    "                Path(self.outputs_dir, f'13_imerg_prec_{p}_{rt}_cv.png'),\n",
    "                bounds=bounds[-1],\n",
    "                shp_path=argentina_shp_path,\n",
    "                elevation_mask=elevation_mask,\n",
    "                cb_label=cb_label[-1],\n",
    "            )\n",
    "        conf_int(\n",
    "            np.array([\n",
    "                self.stations[j][0].result[ind_k]['lognorm'].ppf(T)\n",
    "                for j in range(len(self.stations))\n",
    "                if self.for_use_w_out[j]\n",
    "            ]),\n",
    "            Path(self.outputs_dir, f'14_conf_int.png'),\n",
    "        )\n",
    "        map_error(\n",
    "            self.bbox,\n",
    "            np.array(\n",
    "                [_[0].lon for _ in self.stations]\n",
    "            )[self.for_use_w_out],\n",
    "            np.array(\n",
    "                [_[0].lat for _ in self.stations]\n",
    "            )[self.for_use_w_out],\n",
    "            np.array(\n",
    "                [_[0].institution for _ in self.stations]\n",
    "            )[self.for_use_w_out],\n",
    "            self.test_w_out[self.for_use_w_out],\n",
    "            self.error[self.for_use_w_out],\n",
    "            Path(self.outputs_dir, '15_mape_imerg.png'),\n",
    "        )\n",
    "        comp_result(\n",
    "            self.stations, self.for_use_w_out, self.test_w_out,\n",
    "            Path(self.outputs_dir, '15_comp_imerg.png'),\n",
    "        )\n",
    "        comp_variables(\n",
    "            np.array(\n",
    "                [_[0].institution for _ in self.stations]\n",
    "            )[self.for_use_w_out],\n",
    "            np.array(\n",
    "                [_[0].lon for _ in self.stations]\n",
    "            )[self.for_use_w_out],\n",
    "            np.array(\n",
    "                [_[0].lat for _ in self.stations]\n",
    "            )[self.for_use_w_out],\n",
    "            np.array(\n",
    "                [_[0].elevation for _ in self.stations]\n",
    "            )[self.for_use_w_out],\n",
    "            self.test_w_out[self.for_use_w_out],\n",
    "            self.error[self.for_use_w_out],\n",
    "            Path(self.outputs_dir, '15_comp_vars_imerg.png'),\n",
    "        )\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        ind_k = 'Y_rx1day'\n",
    "        for p in ['Early', 'Late', 'Final']:\n",
    "            save_path = Path(self.outputs_dir, f'imerg_{p}.risa')\n",
    "            if not os.path.exists(save_path):\n",
    "                data_dict = {\n",
    "                    'lon': program.lon_IMERG,\n",
    "                    'lat': program.lat_IMERG,\n",
    "                    'T': program.T,\n",
    "                }\n",
    "                tests = program.imerg[p][12].result[ind_k]['tests_data']\n",
    "                cond_iWW = tests['iWW']['index'].T == 3\n",
    "                cond_hPE = tests['hPE']['index'].T == '3'\n",
    "                cond_tMK = tests['tMK']['index'][:, :, 0].T == 2\n",
    "                data_dict[f'{p}_tests'] = (cond_iWW * cond_hPE * cond_tMK).astype(bool)\n",
    "                data_dict[p] = program.imerg[p][12].result[ind_k]['lognorm']\n",
    "                with open(save_path, 'wb') as f:\n",
    "                    pickle.dump(data_dict, f)\n",
    "\n",
    "    def execute(\n",
    "            self, stations_dir, imerg_dir, rg_bin_dir, imerg_bin_dir,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        ind_k = 'Y_rx1day'\n",
    "        logn_k = 'lognorm'\n",
    "        out_k = 'outliers'\n",
    "        wo_k = 'data_without_outliers'\n",
    "        self.create_paths(stations_dir, rg_bin_dir, imerg_bin_dir)\n",
    "        self.topo = bin_file(Path(self.bin_dir, 'elev.topo'), self.get_topo)\n",
    "        self.imerg = {\n",
    "            p: {\n",
    "                h: get_imerg_grid(\n",
    "                    Path(imerg_dir, f'IMERG_{p}_{h}'),\n",
    "                    Path(self.imerg_bin_dir, f'IMERG_{p}_{h}.pickle'),\n",
    "                    self.T,\n",
    "                    dt_lims=[self.start_date, self.end_date],\n",
    "                    save=True, analyze=True,\n",
    "                ) for h in [3, 12]\n",
    "            } for p in ['Early', 'Late', 'Final']\n",
    "        }\n",
    "        for file in tqdm(os.listdir(self.stations_dir)):\n",
    "            print(file)\n",
    "            name = file[:file.find('.')]\n",
    "            bin_ = bin_file(\n",
    "                Path(self.rg_bin_dir, f'{name}.pickle'),\n",
    "                self.get_analysis, [file, self.imerg],\n",
    "            )\n",
    "            if bin_ is not None:\n",
    "                station, imerg_e, imerg_l, imerg_f = bin_\n",
    "                self.stations.append([station, imerg_e, imerg_l, imerg_f])\n",
    "                test_w_out = [\n",
    "                    freq_conditions(station), freq_conditions(imerg_e),\n",
    "                    freq_conditions(imerg_l), freq_conditions(imerg_f),\n",
    "                ]\n",
    "                self.for_use_w_out.append(\n",
    "                    test_w_out[0] and (\n",
    "                        test_w_out[1] or test_w_out[2] or test_w_out[3]\n",
    "                    )\n",
    "                )\n",
    "                self.test_w_out.append(test_w_out)\n",
    "                test = [\n",
    "                    freq_conditions(station, w_out=False),\n",
    "                    freq_conditions(imerg_e, w_out=False),\n",
    "                    freq_conditions(imerg_l, w_out=False),\n",
    "                    freq_conditions(imerg_f, w_out=False),\n",
    "                ]\n",
    "                self.for_use.append(\n",
    "                    test[0] and (test[1] or test[2] or test[3])\n",
    "                )\n",
    "                self.test.append(test)\n",
    "                self.error.append([\n",
    "                    Card_IMERG.error(\n",
    "                        station.result[ind_k][logn_k].ppf(self.T)[1],\n",
    "                        imerg_e.result[ind_k][logn_k].ppf(self.T)[1],\n",
    "                    ),\n",
    "                    Card_IMERG.error(\n",
    "                        station.result[ind_k][logn_k].ppf(self.T)[1],\n",
    "                        imerg_l.result[ind_k][logn_k].ppf(self.T)[1],\n",
    "                    ),\n",
    "                    Card_IMERG.error(\n",
    "                        station.result[ind_k][logn_k].ppf(self.T)[1],\n",
    "                        imerg_f.result[ind_k][logn_k].ppf(self.T)[1],\n",
    "                    ),\n",
    "                ])\n",
    "                if not test_w_out[1]:\n",
    "                    imerg_e.result[ind_k][out_k][wo_k][-1][:] = np.nan\n",
    "                if not test_w_out[2]:\n",
    "                    imerg_l.result[ind_k][out_k][wo_k][-1][:] = np.nan\n",
    "                if not test_w_out[3]:\n",
    "                    imerg_f.result[ind_k][out_k][wo_k][-1][:] = np.nan\n",
    "                if not test[1]:\n",
    "                    imerg_e.result[ind_k]['data'][:] = np.nan\n",
    "                if not test[2]:\n",
    "                    imerg_l.result[ind_k]['data'][:] = np.nan\n",
    "                if not test[3]:\n",
    "                    imerg_f.result[ind_k]['data'][:] = np.nan\n",
    "                self.create_card(\n",
    "                    station, self.card_dir, imerg_e, imerg_l, imerg_f,\n",
    "                )\n",
    "            clear_output()\n",
    "        self.test_w_out = np.array(self.test_w_out)\n",
    "        self.test = np.array(self.test)\n",
    "        self.for_use_w_out = np.array(self.for_use_w_out, dtype=bool)\n",
    "        self.for_use = np.array(self.for_use, dtype=bool)\n",
    "        self.error = np.array(self.error)\n",
    "        self.get_results(\n",
    "            Path(\n",
    "                Path(os.getcwd()).parent, 'IGN_data',\n",
    "                'shp', 'argentina', 'pais',\n",
    "            )\n",
    "        )\n",
    "        self.save_results()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    print('Main')\n",
    "    \"\"\"\n",
    "    Some directories paths.\n",
    "    \"\"\"\n",
    "    stations_dir = Path(\n",
    "        os.getcwd(), 'inputs', 'rain_gauge', 'preprocess_csv',\n",
    "    )\n",
    "    imerg_dir = Path(os.getcwd(), 'inputs', 'gridded')\n",
    "    rg_bin_dir = 'rain_gauge'\n",
    "    imerg_bin_dir = 'imerg_grid_analyze'\n",
    "    \"\"\"\n",
    "    Run preprocess of rain gauges.\n",
    "    \"\"\"\n",
    "    # run_preprocess(Path('rain_gauge'))\n",
    "    \"\"\"\n",
    "    Main object.\n",
    "    \"\"\"\n",
    "    program = Main()\n",
    "    \"\"\"\n",
    "    Command to download IMERG half-hourly data.\n",
    "    ***Becareful, it takes around 2 months***\n",
    "    \"\"\"\n",
    "    # program.download(Path('f:'))\n",
    "    \"\"\"\n",
    "    The following commands are made to create the daily files of IMERG\n",
    "    half-hourly data.\n",
    "    ***Becareful, a lot of storage needed.***\n",
    "    \"\"\"\n",
    "    # program.gridded_daily_rainfall(\n",
    "    #     Path('f:', 'temp_Early_30T'), Path('m:'), 3, 'Early',\n",
    "    # )\n",
    "    # program.gridded_daily_rainfall(\n",
    "    #     Path('f:', 'temp_Early_30T'), Path('m:'), 12, 'Early',\n",
    "    # )\n",
    "    # program.gridded_daily_rainfall(\n",
    "    #     Path('f:', 'temp_Late_30T'), Path('m:'), 3, 'Late',\n",
    "    # )\n",
    "    # program.gridded_daily_rainfall(\n",
    "    #     Path('f:', 'temp_Late_30T'), Path('m:'), 12, 'Late',\n",
    "    # )\n",
    "    # program.gridded_daily_rainfall(\n",
    "    #     Path('f:', 'temp_Final_30T'), Path('m:'), 3, 'Final',\n",
    "    # )\n",
    "    # program.gridded_daily_rainfall(\n",
    "    #     Path('f:', 'temp_Final_30T'), Path('m:'), 12, 'Final',\n",
    "    # )\n",
    "    \"\"\"\n",
    "    Execution of the program.\n",
    "    ***Becareful, it takes around 12 hours***\n",
    "    \"\"\"\n",
    "    program.execute(\n",
    "        stations_dir, imerg_dir, rg_bin_dir, imerg_bin_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_design_rainfall(\n",
    "    'Early', T=10, lon=-60, lat=-35,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropnan(data):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return data[~np.isnan(data)]\n",
    "\n",
    "def outliers_result(stations):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    ind_k = 'Y_rx1day'\n",
    "    wo_k = 'data_without_outliers'\n",
    "    low = np.array([[dropnan(__.result[ind_k]['outliers']['low_outliers']).shape[0] for __ in _] for _ in stations], dtype=float)\n",
    "    high = np.array([[dropnan(__.result[ind_k]['outliers']['high_outliers']).shape[0] for __ in _] for _ in stations], dtype=float)\n",
    "    print('Number of low outliers')\n",
    "    print(np.array([[np.sum(low[:, j] * low[:, i]) for i in range(4)] for j in range(4)]))\n",
    "    print('Number of high outliers')\n",
    "    print(np.array([[np.sum(high[:, j] * high[:, i]) for i in range(4)] for j in range(4)]))\n",
    "    aux = np.array([[dropnan(__.result[ind_k]['data']).shape[0] for __ in _] for _ in stations]) == np.array([[dropnan(__.result[ind_k]['outliers'][wo_k][-1]).shape[0] for __ in _] for _ in stations])\n",
    "    print('Number of replacements:', np.sum(aux * high * ~low.astype(bool)))\n",
    "\n",
    "def tests_result(obj):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    print('For use:', np.sum(obj.for_use))\n",
    "    print('For use without outliers:', np.sum(obj.for_use_w_out))\n",
    "    institution = np.array([_[0].institution for _ in obj.stations])\n",
    "    for inst in np.unique(institution):\n",
    "        print(\n",
    "            inst, '\\t',\n",
    "            np.sum(institution[~obj.for_use_w_out] == inst),\n",
    "            '\\t', np.sum(institution == inst),\n",
    "            '\\t', 100 * np.sum(institution[~obj.for_use_w_out] == inst) / np.sum(institution == inst),\n",
    "        )\n",
    "\n",
    "outliers_result(program.stations)\n",
    "tests_result(program)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "5bb324ccc0b743af94823599a3eaa9effecaa98bc173db50f15ace86d7c4841b"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "89d622a19b21b4ec92455ea2727815412b79697fbd1c0390894ebb53ab003468"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
